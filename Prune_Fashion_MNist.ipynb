{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prune Fashion MNist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFFWJOUOjOMsrGe5zj3NEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashdeepjassal/Pruning/blob/main/Prune_Fashion_MNist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K413s_vqEwLy"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn, optim\n",
        "from numpy import linalg as LA\n",
        "from scipy.stats import rankdata\n",
        "from collections import OrderedDict\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6wB78NwSyLW"
      },
      "source": [
        "### Some Sources\n",
        "* Major Pruning Papers https://github.com/chenbong/awesome-compression-papers\n",
        "* https://www.tensorflow.org/model_optimization/api_docs/python/tfmot\n",
        "* https://jacobgil.github.io/deeplearning/pruning-deep-learning\n",
        "* https://heartbeat.fritz.ai/research-guide-pruning-techniques-for-neural-networks-d9b8440ab10d#:~:text=Pruning%20is%20a%20technique%20in,values%20in%20the%20weight%20tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp85eygFE1rX"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#This is similar to LeNet by LeCun et al., 1998.\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "model = MyNet().to(device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx_pEHHGFauC"
      },
      "source": [
        "## Inspect Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqc31nNyFILU"
      },
      "source": [
        "module = model.conv1\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622jM8tTFgPF"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az8hGW-TFlzV"
      },
      "source": [
        "## Let's Prune a module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdrULvlsFjDP"
      },
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBBwOcrHFs8g"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LnPoyG8FyTT"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boe2QcoUF1Oe"
      },
      "source": [
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWl9kG56F_de"
      },
      "source": [
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XpKksY0GBYy"
      },
      "source": [
        "prune.l1_unstructured(module, name=\"bias\", amount=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghO2I8p2GB6N"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPDqw3zmGEYD"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f53krMKGF5U"
      },
      "source": [
        "print(module.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm9O8stUGH5z"
      },
      "source": [
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BHoBoQWGSw_"
      },
      "source": [
        "## Iterative Pruning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpVzWmeJGU2O"
      },
      "source": [
        "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
        "\n",
        "# As we can verify, this will zero out all the connections corresponding to\n",
        "# 50% (3 out of 6) of the channels, while preserving the action of the\n",
        "# previous mask.\n",
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdl9TMi2GZtf"
      },
      "source": [
        "for hook in module._forward_pre_hooks.values():\n",
        "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
        "        break\n",
        "\n",
        "print(list(hook))  # pruning history in the container"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GhyFpNNGepz"
      },
      "source": [
        "## Serialization of a pruned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BCtI4xIGdmQ"
      },
      "source": [
        "print(model.state_dict().keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw7NlTu1GmeT"
      },
      "source": [
        "## Remove pruning re-parametrization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju-dok4RGkP1"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9rIz_a9GkUz"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHDoZ1eeGkcU"
      },
      "source": [
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfk3GWUHGka8"
      },
      "source": [
        "prune.remove(module, 'weight')\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfsqdqMWGkYv"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hzxi26ZGxnV"
      },
      "source": [
        "## Pruning multiple parameters in a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDiFkMqKGkTH"
      },
      "source": [
        "new_model = MyNet()\n",
        "for name, module in new_model.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "print(dict(new_model.named_buffers()).keys())  # to verify that all masks exist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L8CFxQMG5Dj"
      },
      "source": [
        "## Global pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSMub4x7G6Fz"
      },
      "source": [
        "model = MyNet()\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    # (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3PrrlovHBYG"
      },
      "source": [
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "# print(\n",
        "#     \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "#         100. * float(torch.sum(model.fc3.weight == 0))\n",
        "#         / float(model.fc3.weight.nelement())\n",
        "#     )\n",
        "# )\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            # + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            # + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bAAlTXtHK2z"
      },
      "source": [
        "##  Now Let's train a model and see prunnning in action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "324MTckKHIcr"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7buCaG7GN5F9"
      },
      "source": [
        "##Let's try prunning with a Dense model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7e4lP5vNqYb"
      },
      "source": [
        "\n",
        "class MyNet1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 1000, bias=False)\n",
        "        self.fc2 = nn.Linear(1000, 1000, bias=False)\n",
        "        self.fc3 = nn.Linear(1000, 500, bias=False)\n",
        "        self.fc4 = nn.Linear(500, 200, bias=False)\n",
        "        self.fc5 = nn.Linear(200, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.log_softmax(self.fc5(x), dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfirIifUMuPl"
      },
      "source": [
        "model = MyNet1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX15eYBXHTzc"
      },
      "source": [
        "\n",
        "# required variables \n",
        "import numpy as np\n",
        "batch_size = 128  # batch size\n",
        "valid_size = 0.2  # validation set size: 20%\n",
        "epochs = 20  # number of epochs\n",
        "min_valid_loss = np.Inf  # min value for validation loss\n",
        "transform = transforms.ToTensor()  # necessary image transform i.e. conver tot Tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48pmcxxYHU3x"
      },
      "source": [
        "# Download the Train and Test set\n",
        "\n",
        "trainset = datasets.FashionMNIST('.', train=True, download=True, transform=transform)\n",
        "testset = datasets.FashionMNIST('.', train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kdxeNh4HZQS"
      },
      "source": [
        "\n",
        "# Split the training set indices into training and validation set indices using 80:20 ratio\n",
        "\n",
        "len_trainset = len(trainset)\n",
        "index_list = list(range(len_trainset))\n",
        "np.random.shuffle(index_list)\n",
        "split_index = int(len_trainset * valid_size)\n",
        "train_indices, valid_indices = index_list[split_index:], index_list[:split_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH56OSM6Hbuy"
      },
      "source": [
        "\n",
        "# Create Samplers for training and validation set using the indices\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(valid_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz5G9M8bHdeP"
      },
      "source": [
        "# Create loaders for train, validation and testing set using samplers\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
        "validloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23jWVFbhHfe_"
      },
      "source": [
        "model=MyNet1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpK2avr9Hgma"
      },
      "source": [
        "\n",
        "# Using CrossEntropy as my Loss function and Adam Optimizer with a learning rate of 0.01 \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QXQuerSHkv7"
      },
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    model.train()\n",
        "    # Training\n",
        "    for image, label in trainloader:\n",
        "        # Set Gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass the image through the network\n",
        "        output = model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "        # Train loss is the loss multiplied by the batch size\n",
        "        train_loss += loss.item() * image.size(0)\n",
        "    model.eval()\n",
        "    # Validation\n",
        "    for image, label in validloader:\n",
        "        # forward pass the image through the network\n",
        "        output = model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Validation loss is the loss multiplied by the batch size\n",
        "        valid_loss += loss.item() * image.size(0)\n",
        "\n",
        "    # Total train loss is the average of train loss, same for validation loss\n",
        "    train_loss = train_loss/len(trainloader.sampler)\n",
        "    valid_loss = valid_loss/len(validloader.sampler)\n",
        "  \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "      epoch+1, \n",
        "      train_loss,\n",
        "      valid_loss\n",
        "      ))\n",
        "    \n",
        "    # save model only if validation loss has decreased\n",
        "    if valid_loss <= min_valid_loss:\n",
        "        print('Validation loss decreased ({} --> {}).  Saving model ...'.format(\n",
        "        min_valid_loss,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "        min_valid_loss = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XLmtbU1Hms8"
      },
      "source": [
        "# Function to check test accuracy\n",
        "\n",
        "def test_accuracy(model, testloader, criterion):\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for data, target in testloader:\n",
        "        # forward pass\n",
        "        # data,target=data.to(device),target.to(device)\n",
        "        output = model(data)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, target)\n",
        "        # Test loss \n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class and get the max class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare prediction with true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "    # Overall accuracy\n",
        "    overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
        "    return overall_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-2DLKx2iWwA"
      },
      "source": [
        "initial_accuracy = test_accuracy(model, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y4roVwRHp9m"
      },
      "source": [
        "# Prune ranges provided in the PDF\n",
        "\n",
        "prune_percentage = [.0, .10, .15, .20, .25, .30, .35, .40, .45, .50, .55, .60, .65, .70, .75, .80, .90, .95, .97, .99]\n",
        "accuracies_wp = []\n",
        "accuracies_np = []\n",
        "\n",
        "# Get the accuracy without any pruning \n",
        "initial_accuracy = test_accuracy(model, testloader, criterion)\n",
        "accuracies_wp.append(initial_accuracy)\n",
        "accuracies_np.append(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX6w5d57d99x"
      },
      "source": [
        "## Weight Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nT3Tc_gIfvN"
      },
      "source": [
        " \n",
        "# Loop through each prune percent\n",
        "for k in prune_percentage[1:]:\n",
        "    # Load the original unprunned model\n",
        "    model = MyNet1()\n",
        "    model.load_state_dict(torch.load('model.pt'))\n",
        "    # Get all the weights\n",
        "    weights = model.state_dict()\n",
        "    # Get keys to access model weights\n",
        "    layers = list(model.state_dict())\n",
        "    ranks = {}\n",
        "    pruned_weights = []\n",
        "    # For each layer except the output one\n",
        "    for l in layers[:-1]:\n",
        "        # Get weights for each layer and conver to numpy \n",
        "        data = weights[l]\n",
        "        w = np.array(data)\n",
        "        # Rank the weights element wise and reshape rank elements as the model weights\n",
        "        ranks[l]=(rankdata(np.abs(w), method='dense') - 1).astype(int).reshape(w.shape)\n",
        "        # Get the threshold value based on the value of k(prune percentage) \n",
        "        lower_bound_rank = np.ceil(np.max(ranks[l]) * k).astype(int)\n",
        "        # Assign rank elements to 0 that are less than or equal to the threshold and 1 to those that are above.\n",
        "        ranks[l][ranks[l] <= lower_bound_rank] = 0\n",
        "        ranks[l][ranks[l] > lower_bound_rank] = 1\n",
        "        # Multiply weights array with ranks to zero out the lower ranked weights\n",
        "        w = w * ranks[l]\n",
        "        # Assign the updated weights as tensor to data and append to the pruned_weights list \n",
        "        data[...] = torch.from_numpy(w)\n",
        "        pruned_weights.append(data)\n",
        "    # Append the last layer weights as it is\n",
        "    pruned_weights.append(weights[layers[-1]])\n",
        "    # Update the model weights with all the updated weights \n",
        "    new_state_dict = OrderedDict()\n",
        "    for l, pw in zip(layers, pruned_weights):\n",
        "        new_state_dict[l] = pw\n",
        "    model.state_dict = new_state_dict\n",
        "    # append the test accuracy to accuracies_wp\n",
        "    accuracies_wp.append(test_accuracy(model, testloader, criterion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_BfYdWBd5uv"
      },
      "source": [
        "## Neuron Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeFYreWuRCaK"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnnUVnwPIkcJ"
      },
      "source": [
        "\n",
        "# Code is almost same as above so comments are provided for only different parts of code\n",
        "dense=True\n",
        "for k in prune_percentage[1:]:\n",
        "    model = MyNet1()\n",
        "    model.load_state_dict(torch.load('model.pt'))\n",
        "    weights = model.state_dict()\n",
        "    layers = list(model.state_dict())\n",
        "    ranks = {}\n",
        "    pruned_weights = []\n",
        "    for l in layers[:-1]:\n",
        "        data = weights[l]\n",
        "        w = np.array(data)\n",
        "        # taking norm for each neuron\n",
        "        norm = LA.norm(w, axis=0)\n",
        "        # repeat the norm values to get the shape similar to that of layer weights\n",
        "        norm = np.tile(norm, (w.shape[0],1))\n",
        "        ranks[l] = (rankdata(norm, method='dense') - 1).astype(int).reshape(norm.shape)\n",
        "        lower_bound_rank = np.ceil(np.max(ranks[l])*k).astype(int)\n",
        "        ranks[l][ranks[l] <= lower_bound_rank] = 0\n",
        "        ranks[l][ranks[l] > lower_bound_rank] = 1\n",
        "        if dense:\n",
        "          w = w * ranks[l]\n",
        "        data[...] = torch.from_numpy(w)\n",
        "        pruned_weights.append(data)\n",
        "    pruned_weights.append(weights[layers[-1]])\n",
        "    new_state_dict = OrderedDict()\n",
        "    for l, pw in zip(layers, pruned_weights):\n",
        "        new_state_dict[l] = pw\n",
        "    model.state_dict = new_state_dict\n",
        "    accuracies_np.append(test_accuracy(model, testloader, criterion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwLT-akXImVt"
      },
      "source": [
        "# Plot the sparsity vs accuracy graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(prune_percentage, accuracies_np, label='Neuron Pruning')\n",
        "plt.plot(prune_percentage, accuracies_wp, label='Weight Pruning')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('% sparsity')\n",
        "plt.ylabel('% accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBo8lyX4UFfJ"
      },
      "source": [
        "# Prune Conv2D model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nv8WZqEijm_"
      },
      "source": [
        "# Function to check test accuracy\n",
        "\n",
        "def test_accuracy(model, testloader, criterion):\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for data, target in testloader:\n",
        "        # forward pass\n",
        "        data,target=data.to(device),target.to(device)\n",
        "        output = model(data)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, target)\n",
        "        # Test loss \n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class and get the max class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare prediction with true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "    # Overall accuracy\n",
        "    overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
        "    return overall_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfPc1UWSg1S"
      },
      "source": [
        "# new_model\n",
        "new_model = MyNet().to(device=device)\n",
        "\n",
        "\n",
        "# Using CrossEntropy as my Loss function and Adam Optimizer with a learning rate of 0.01 \n",
        "batch_size = 128  # batch size\n",
        "valid_size = 0.2  # validation set size: 20%\n",
        "epochs = 50  # number of epochs\n",
        "min_valid_loss = np.Inf  # min value for validation loss\n",
        "transform = transforms.ToTensor()  # necessary image transform i.e. conver tot Tensor\n",
        "momentum = 0.9\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(new_model.parameters(), lr=0.001,\n",
        "                      momentum=momentum)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    new_model.train()\n",
        "    # Training\n",
        "    for image, label in trainloader:\n",
        "        # Set Gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        image, label=image.to(device),label.to(device)\n",
        "        # forward pass the image through the network\n",
        "        output = new_model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "        # Train loss is the loss multiplied by the batch size\n",
        "        train_loss += loss.item() * image.size(0)\n",
        "    new_model.eval()\n",
        "    # Validation\n",
        "    for image, label in validloader:\n",
        "        # forward pass the image through the network\n",
        "        image, label=image.to(device),label.to(device)\n",
        "        output = new_model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Validation loss is the loss multiplied by the batch size\n",
        "        valid_loss += loss.item() * image.size(0)\n",
        "\n",
        "    # Total train loss is the average of train loss, same for validation loss\n",
        "    train_loss = train_loss/len(trainloader.sampler)\n",
        "    valid_loss = valid_loss/len(validloader.sampler)\n",
        "  \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "      epoch+1, \n",
        "      train_loss,\n",
        "      valid_loss\n",
        "      ))\n",
        "    \n",
        "    # save model only if validation loss has decreased\n",
        "    if valid_loss <= min_valid_loss:\n",
        "        print('Validation loss decreased ({} --> {}).  Saving model ...'.format(\n",
        "        min_valid_loss,\n",
        "        valid_loss))\n",
        "        torch.save(new_model.state_dict(), 'model1.pt')\n",
        "        min_valid_loss = valid_loss\n",
        "    initial_accuracy = test_accuracy(new_model, testloader, criterion)\n",
        "    print(initial_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFbLcyPOcT1z"
      },
      "source": [
        "initial_accuracy = test_accuracy(new_model, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMF-vH6SUSBn"
      },
      "source": [
        "# Test Accuracy after Pruning multiple parameters in a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpT55d5-TUlp"
      },
      "source": [
        "new_model1= MyNet().to(device=device)\n",
        "# new_model1=new_model\n",
        "new_model1.load_state_dict(torch.load('model1.pt'))\n",
        "for name, module in new_model1.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.01)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.02)\n",
        "\n",
        "print(dict(new_model1.named_buffers()).keys())  # to verify that all masks exist\n",
        "initial_accuracy = test_accuracy(new_model1, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WtBSw5Rf6zs"
      },
      "source": [
        "new_model1= MyNet().to(device=device)\n",
        "# new_model1=new_model\n",
        "new_model1.load_state_dict(torch.load('model1.pt'))\n",
        "for name, module in new_model1.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.1)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "\n",
        "print(dict(new_model1.named_buffers()).keys())  # to verify that all masks exist\n",
        "initial_accuracy = test_accuracy(new_model1, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ogC0aCnf9YD"
      },
      "source": [
        "new_model1= MyNet().to(device=device)\n",
        "# new_model1=new_model\n",
        "new_model1.load_state_dict(torch.load('model1.pt'))\n",
        "for name, module in new_model1.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "print(dict(new_model1.named_buffers()).keys())  # to verify that all masks exist\n",
        "initial_accuracy = test_accuracy(new_model1, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDIctulWUTz"
      },
      "source": [
        "# Test after global prunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDACzU32i0Rr"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dphw3MFi4Et"
      },
      "source": [
        "torch.cuda.empty_cache() # PyTorch thing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqgwCzriWNcq"
      },
      "source": [
        "new_model2= MyNet().to(device=device)\n",
        "new_model2.load_state_dict(torch.load('model1.pt'))\n",
        "# new_model2=new_model\n",
        "model = new_model2\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    # (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.15,   \n",
        ")\n",
        "initial_accuracy = test_accuracy(new_model2, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2UucLmsWfI6"
      },
      "source": [
        "\n",
        "new_model2= MyNet().to(device=device)\n",
        "new_model2.load_state_dict(torch.load('model1.pt'))\n",
        "# new_model2=new_model\n",
        "model = new_model2\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    # (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.25,   \n",
        ")\n",
        "initial_accuracy = test_accuracy(new_model2, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JL5ak32lWwc",
        "outputId": "51c91ed7-46ae-40dc-e2d1-5381647777ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        }
      },
      "source": [
        "prune_percentage = [0.01,0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80, 0.90, 0.95, 0.97, 0.99]\n",
        "index=[x for x in range(len(prune_percentage))]\n",
        "accuracies_wp = []\n",
        "accuracies_np = []\n",
        "\n",
        "for k in prune_percentage:\n",
        "    # model = MyNet()\n",
        "    # model.load_state_dict(torch.load('model1.pt'))\n",
        "    new_model2= MyNet().to(device=device) \n",
        "    new_model2.load_state_dict(torch.load('model1.pt'))\n",
        "    model=new_model2\n",
        "    weights = model.state_dict()\n",
        "    parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    # (model.fc3, 'weight'),\n",
        "    )\n",
        "    prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=k,   \n",
        "    )\n",
        "    initial_accuracy = test_accuracy(model, testloader, criterion)\n",
        "    print(k,\":\",initial_accuracy)\n",
        "    accuracies_np.append(test_accuracy(model, testloader, criterion))\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.01 : 82.26\n",
            "0.1 : 82.35\n",
            "0.15 : 82.29\n",
            "0.2 : 82.25\n",
            "0.25 : 81.96\n",
            "0.3 : 81.9\n",
            "0.35 : 82.0\n",
            "0.4 : 82.19\n",
            "0.45 : 82.01\n",
            "0.5 : 81.95\n",
            "0.55 : 81.88\n",
            "0.6 : 82.09\n",
            "0.65 : 80.69\n",
            "0.7 : 79.72\n",
            "0.75 : 78.62\n",
            "0.8 : 75.65\n",
            "0.9 : 36.19\n",
            "0.95 : 12.37\n",
            "0.97 : 10.03\n",
            "0.99 : 10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4KwzpRLlxWR",
        "outputId": "85890880-f915-4e0e-9371-0d2155ff9957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "# Plot the sparsity vs accuracy graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(prune_percentage, accuracies_np, label='Global Pruning')\n",
        "# plt.plot(prune_percentage, accuracies_wp, label='Weight Pruning')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('% sparsity')\n",
        "plt.ylabel('% accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBN87xZWl8aA"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}