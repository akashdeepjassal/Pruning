{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prune Fashion MNist.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOBhp39PKm5PAFoWWDWS9Qi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akashdeepjassal/Pruning/blob/main/Prune_Fashion_MNist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K413s_vqEwLy"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn, optim\n",
        "from numpy import linalg as LA\n",
        "from scipy.stats import rankdata\n",
        "from collections import OrderedDict\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6wB78NwSyLW"
      },
      "source": [
        "### Some Sources\n",
        "* Major Pruning Papers https://github.com/chenbong/awesome-compression-papers\n",
        "* https://www.tensorflow.org/model_optimization/api_docs/python/tfmot\n",
        "* https://jacobgil.github.io/deeplearning/pruning-deep-learning\n",
        "* https://heartbeat.fritz.ai/research-guide-pruning-techniques-for-neural-networks-d9b8440ab10d#:~:text=Pruning%20is%20a%20technique%20in,values%20in%20the%20weight%20tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp85eygFE1rX"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "#This is similar to LeNet by LeCun et al., 1998.\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
        "        self.conv2_drop = nn.Dropout2d()\n",
        "        self.fc1 = nn.Linear(320, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "        x = x.view(-1, 320)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)\n",
        "model = MyNet().to(device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx_pEHHGFauC"
      },
      "source": [
        "## Inspect Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqc31nNyFILU"
      },
      "source": [
        "module = model.conv1\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "622jM8tTFgPF"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az8hGW-TFlzV"
      },
      "source": [
        "## Let's Prune a module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdrULvlsFjDP"
      },
      "source": [
        "prune.random_unstructured(module, name=\"weight\", amount=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBBwOcrHFs8g"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LnPoyG8FyTT"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boe2QcoUF1Oe"
      },
      "source": [
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWl9kG56F_de"
      },
      "source": [
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XpKksY0GBYy"
      },
      "source": [
        "prune.l1_unstructured(module, name=\"bias\", amount=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghO2I8p2GB6N"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPDqw3zmGEYD"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f53krMKGF5U"
      },
      "source": [
        "print(module.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm9O8stUGH5z"
      },
      "source": [
        "print(module._forward_pre_hooks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BHoBoQWGSw_"
      },
      "source": [
        "## Iterative Pruning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpVzWmeJGU2O"
      },
      "source": [
        "prune.ln_structured(module, name=\"weight\", amount=0.5, n=2, dim=0)\n",
        "\n",
        "# As we can verify, this will zero out all the connections corresponding to\n",
        "# 50% (3 out of 6) of the channels, while preserving the action of the\n",
        "# previous mask.\n",
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fdl9TMi2GZtf"
      },
      "source": [
        "for hook in module._forward_pre_hooks.values():\n",
        "    if hook._tensor_name == \"weight\":  # select out the correct hook\n",
        "        break\n",
        "\n",
        "print(list(hook))  # pruning history in the container"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GhyFpNNGepz"
      },
      "source": [
        "## Serialization of a pruned model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BCtI4xIGdmQ"
      },
      "source": [
        "print(model.state_dict().keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw7NlTu1GmeT"
      },
      "source": [
        "## Remove pruning re-parametrization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ju-dok4RGkP1"
      },
      "source": [
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9rIz_a9GkUz"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHDoZ1eeGkcU"
      },
      "source": [
        "print(module.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfk3GWUHGka8"
      },
      "source": [
        "prune.remove(module, 'weight')\n",
        "print(list(module.named_parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfsqdqMWGkYv"
      },
      "source": [
        "print(list(module.named_buffers()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hzxi26ZGxnV"
      },
      "source": [
        "## Pruning multiple parameters in a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDiFkMqKGkTH"
      },
      "source": [
        "new_model = MyNet()\n",
        "for name, module in new_model.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "print(dict(new_model.named_buffers()).keys())  # to verify that all masks exist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L8CFxQMG5Dj"
      },
      "source": [
        "## Global pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSMub4x7G6Fz"
      },
      "source": [
        "model = MyNet()\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    # (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.2,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3PrrlovHBYG"
      },
      "source": [
        "print(\n",
        "    \"Sparsity in conv1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv1.weight == 0))\n",
        "        / float(model.conv1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in conv2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.conv2.weight == 0))\n",
        "        / float(model.conv2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc1.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc1.weight == 0))\n",
        "        / float(model.fc1.weight.nelement())\n",
        "    )\n",
        ")\n",
        "print(\n",
        "    \"Sparsity in fc2.weight: {:.2f}%\".format(\n",
        "        100. * float(torch.sum(model.fc2.weight == 0))\n",
        "        / float(model.fc2.weight.nelement())\n",
        "    )\n",
        ")\n",
        "# print(\n",
        "#     \"Sparsity in fc3.weight: {:.2f}%\".format(\n",
        "#         100. * float(torch.sum(model.fc3.weight == 0))\n",
        "#         / float(model.fc3.weight.nelement())\n",
        "#     )\n",
        "# )\n",
        "print(\n",
        "    \"Global sparsity: {:.2f}%\".format(\n",
        "        100. * float(\n",
        "            torch.sum(model.conv1.weight == 0)\n",
        "            + torch.sum(model.conv2.weight == 0)\n",
        "            + torch.sum(model.fc1.weight == 0)\n",
        "            + torch.sum(model.fc2.weight == 0)\n",
        "            # + torch.sum(model.fc3.weight == 0)\n",
        "        )\n",
        "        / float(\n",
        "            model.conv1.weight.nelement()\n",
        "            + model.conv2.weight.nelement()\n",
        "            + model.fc1.weight.nelement()\n",
        "            + model.fc2.weight.nelement()\n",
        "            # + model.fc3.weight.nelement()\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bAAlTXtHK2z"
      },
      "source": [
        "##  Now Let's train a model and see prunnning in action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "324MTckKHIcr"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7buCaG7GN5F9"
      },
      "source": [
        "##Let's try prunning with a Dense model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7e4lP5vNqYb"
      },
      "source": [
        "\n",
        "class MyNet1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 1000, bias=False)\n",
        "        self.fc2 = nn.Linear(1000, 1000, bias=False)\n",
        "        self.fc3 = nn.Linear(1000, 500, bias=False)\n",
        "        self.fc4 = nn.Linear(500, 200, bias=False)\n",
        "        self.fc5 = nn.Linear(200, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = F.log_softmax(self.fc5(x), dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfirIifUMuPl"
      },
      "source": [
        "model = MyNet1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX15eYBXHTzc"
      },
      "source": [
        "\n",
        "# required variables \n",
        "import numpy as np\n",
        "batch_size = 128  # batch size\n",
        "valid_size = 0.2  # validation set size: 20%\n",
        "epochs = 20  # number of epochs\n",
        "min_valid_loss = np.Inf  # min value for validation loss\n",
        "transform = transforms.ToTensor()  # necessary image transform i.e. conver tot Tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48pmcxxYHU3x"
      },
      "source": [
        "# Download the Train and Test set\n",
        "\n",
        "trainset = datasets.FashionMNIST('.', train=True, download=True, transform=transform)\n",
        "testset = datasets.FashionMNIST('.', train=False, download=True, transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kdxeNh4HZQS"
      },
      "source": [
        "\n",
        "# Split the training set indices into training and validation set indices using 80:20 ratio\n",
        "\n",
        "len_trainset = len(trainset)\n",
        "index_list = list(range(len_trainset))\n",
        "np.random.shuffle(index_list)\n",
        "split_index = int(len_trainset * valid_size)\n",
        "train_indices, valid_indices = index_list[split_index:], index_list[:split_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kH56OSM6Hbuy"
      },
      "source": [
        "\n",
        "# Create Samplers for training and validation set using the indices\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(valid_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz5G9M8bHdeP"
      },
      "source": [
        "# Create loaders for train, validation and testing set using samplers\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\n",
        "validloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23jWVFbhHfe_"
      },
      "source": [
        "model=MyNet1()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpK2avr9Hgma"
      },
      "source": [
        "\n",
        "# Using CrossEntropy as my Loss function and Adam Optimizer with a learning rate of 0.01 \n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QXQuerSHkv7"
      },
      "source": [
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    model.train()\n",
        "    # Training\n",
        "    for image, label in trainloader:\n",
        "        # Set Gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass the image through the network\n",
        "        output = model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "        # Train loss is the loss multiplied by the batch size\n",
        "        train_loss += loss.item() * image.size(0)\n",
        "    model.eval()\n",
        "    # Validation\n",
        "    for image, label in validloader:\n",
        "        # forward pass the image through the network\n",
        "        output = model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Validation loss is the loss multiplied by the batch size\n",
        "        valid_loss += loss.item() * image.size(0)\n",
        "\n",
        "    # Total train loss is the average of train loss, same for validation loss\n",
        "    train_loss = train_loss/len(trainloader.sampler)\n",
        "    valid_loss = valid_loss/len(validloader.sampler)\n",
        "  \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "      epoch+1, \n",
        "      train_loss,\n",
        "      valid_loss\n",
        "      ))\n",
        "    \n",
        "    # save model only if validation loss has decreased\n",
        "    if valid_loss <= min_valid_loss:\n",
        "        print('Validation loss decreased ({} --> {}).  Saving model ...'.format(\n",
        "        min_valid_loss,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model.pt')\n",
        "        min_valid_loss = valid_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XLmtbU1Hms8"
      },
      "source": [
        "# Function to check test accuracy\n",
        "\n",
        "def test_accuracy(model, testloader, criterion):\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for data, target in testloader:\n",
        "        # forward pass\n",
        "        # data,target=data.to(device),target.to(device)\n",
        "        output = model(data)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, target)\n",
        "        # Test loss \n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class and get the max class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare prediction with true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "    # Overall accuracy\n",
        "    overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
        "    return overall_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-2DLKx2iWwA"
      },
      "source": [
        "initial_accuracy = test_accuracy(model, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y4roVwRHp9m"
      },
      "source": [
        "# Prune ranges provided in the PDF\n",
        "\n",
        "prune_percentage = [.0, .10, .15, .20, .25, .30, .35, .40, .45, .50, .55, .60, .65, .70, .75, .80, .90, .95, .97, .99]\n",
        "accuracies_wp = []\n",
        "accuracies_np = []\n",
        "\n",
        "# Get the accuracy without any pruning \n",
        "initial_accuracy = test_accuracy(model, testloader, criterion)\n",
        "accuracies_wp.append(initial_accuracy)\n",
        "accuracies_np.append(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JX6w5d57d99x"
      },
      "source": [
        "## Weight Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nT3Tc_gIfvN"
      },
      "source": [
        " \n",
        "# Loop through each prune percent\n",
        "for k in prune_percentage[1:]:\n",
        "    # Load the original unprunned model\n",
        "    model = MyNet1()\n",
        "    model.load_state_dict(torch.load('model.pt'))\n",
        "    # Get all the weights\n",
        "    weights = model.state_dict()\n",
        "    # Get keys to access model weights\n",
        "    layers = list(model.state_dict())\n",
        "    ranks = {}\n",
        "    pruned_weights = []\n",
        "    # For each layer except the output one\n",
        "    for l in layers[:-1]:\n",
        "        # Get weights for each layer and conver to numpy \n",
        "        data = weights[l]\n",
        "        w = np.array(data)\n",
        "        # Rank the weights element wise and reshape rank elements as the model weights\n",
        "        ranks[l]=(rankdata(np.abs(w), method='dense') - 1).astype(int).reshape(w.shape)\n",
        "        # Get the threshold value based on the value of k(prune percentage) \n",
        "        lower_bound_rank = np.ceil(np.max(ranks[l]) * k).astype(int)\n",
        "        # Assign rank elements to 0 that are less than or equal to the threshold and 1 to those that are above.\n",
        "        ranks[l][ranks[l] <= lower_bound_rank] = 0\n",
        "        ranks[l][ranks[l] > lower_bound_rank] = 1\n",
        "        # Multiply weights array with ranks to zero out the lower ranked weights\n",
        "        w = w * ranks[l]\n",
        "        # Assign the updated weights as tensor to data and append to the pruned_weights list \n",
        "        data[...] = torch.from_numpy(w)\n",
        "        pruned_weights.append(data)\n",
        "    # Append the last layer weights as it is\n",
        "    pruned_weights.append(weights[layers[-1]])\n",
        "    # Update the model weights with all the updated weights \n",
        "    new_state_dict = OrderedDict()\n",
        "    for l, pw in zip(layers, pruned_weights):\n",
        "        new_state_dict[l] = pw\n",
        "    model.state_dict = new_state_dict\n",
        "    # append the test accuracy to accuracies_wp\n",
        "    accuracies_wp.append(test_accuracy(model, testloader, criterion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_BfYdWBd5uv"
      },
      "source": [
        "## Neuron Pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeFYreWuRCaK"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnnUVnwPIkcJ"
      },
      "source": [
        "\n",
        "# Code is almost same as above so comments are provided for only different parts of code\n",
        "dense=True\n",
        "for k in prune_percentage[1:]:\n",
        "    model = MyNet1()\n",
        "    model.load_state_dict(torch.load('model.pt'))\n",
        "    weights = model.state_dict()\n",
        "    layers = list(model.state_dict())\n",
        "    ranks = {}\n",
        "    pruned_weights = []\n",
        "    for l in layers[:-1]:\n",
        "        data = weights[l]\n",
        "        w = np.array(data)\n",
        "        # taking norm for each neuron\n",
        "        norm = LA.norm(w, axis=0)\n",
        "        # repeat the norm values to get the shape similar to that of layer weights\n",
        "        norm = np.tile(norm, (w.shape[0],1))\n",
        "        ranks[l] = (rankdata(norm, method='dense') - 1).astype(int).reshape(norm.shape)\n",
        "        lower_bound_rank = np.ceil(np.max(ranks[l])*k).astype(int)\n",
        "        ranks[l][ranks[l] <= lower_bound_rank] = 0\n",
        "        ranks[l][ranks[l] > lower_bound_rank] = 1\n",
        "        if dense:\n",
        "          w = w * ranks[l]\n",
        "        data[...] = torch.from_numpy(w)\n",
        "        pruned_weights.append(data)\n",
        "    pruned_weights.append(weights[layers[-1]])\n",
        "    new_state_dict = OrderedDict()\n",
        "    for l, pw in zip(layers, pruned_weights):\n",
        "        new_state_dict[l] = pw\n",
        "    model.state_dict = new_state_dict\n",
        "    accuracies_np.append(test_accuracy(model, testloader, criterion))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwLT-akXImVt"
      },
      "source": [
        "# Plot the sparsity vs accuracy graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(prune_percentage, accuracies_np, label='Neuron Pruning')\n",
        "plt.plot(prune_percentage, accuracies_wp, label='Weight Pruning')\n",
        "plt.legend(loc='best')\n",
        "plt.xlabel('% sparsity')\n",
        "plt.ylabel('% accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBo8lyX4UFfJ"
      },
      "source": [
        "# Prune Conv2D model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Nv8WZqEijm_"
      },
      "source": [
        "# Function to check test accuracy\n",
        "\n",
        "def test_accuracy(model, testloader, criterion):\n",
        "    test_loss = 0.0\n",
        "    class_correct = list(0. for i in range(10))\n",
        "    class_total = list(0. for i in range(10))\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for data, target in testloader:\n",
        "        # forward pass\n",
        "        data,target=data.to(device),target.to(device)\n",
        "        output = model(data)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, target)\n",
        "        # Test loss \n",
        "        test_loss += loss.item()*data.size(0)\n",
        "        # convert output probabilities to predicted class and get the max class\n",
        "        _, pred = torch.max(output, 1)\n",
        "        # compare prediction with true label\n",
        "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
        "        # calculate test accuracy for each object class\n",
        "        for i in range(len(target)):\n",
        "            label = target.data[i]\n",
        "            class_correct[label] += correct[i].item()\n",
        "            class_total[label] += 1\n",
        "    # Overall accuracy\n",
        "    overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)\n",
        "    return overall_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVfPc1UWSg1S"
      },
      "source": [
        "# new_model\n",
        "new_model = MyNet().to(device=device)\n",
        "\n",
        "\n",
        "# Using CrossEntropy as my Loss function and Adam Optimizer with a learning rate of 0.01 \n",
        "batch_size = 128  # batch size\n",
        "valid_size = 0.2  # validation set size: 20%\n",
        "epochs = 100  # number of epochs\n",
        "min_valid_loss = np.Inf  # min value for validation loss\n",
        "transform = transforms.ToTensor()  # necessary image transform i.e. conver tot Tensor\n",
        "momentum = 0.5\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(new_model.parameters(), lr=0.001,\n",
        "                      momentum=momentum)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    new_model.train()\n",
        "    # Training\n",
        "    for image, label in trainloader:\n",
        "        # Set Gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        image, label=image.to(device),label.to(device)\n",
        "        # forward pass the image through the network\n",
        "        output = new_model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "        # Optimization step\n",
        "        optimizer.step()\n",
        "        # Train loss is the loss multiplied by the batch size\n",
        "        train_loss += loss.item() * image.size(0)\n",
        "    new_model.eval()\n",
        "    # Validation\n",
        "    for image, label in validloader:\n",
        "        # forward pass the image through the network\n",
        "        image, label=image.to(device),label.to(device)\n",
        "        output = new_model(image)\n",
        "        # Get the loss\n",
        "        loss = criterion(output, label)\n",
        "        # Validation loss is the loss multiplied by the batch size\n",
        "        valid_loss += loss.item() * image.size(0)\n",
        "\n",
        "    # Total train loss is the average of train loss, same for validation loss\n",
        "    train_loss = train_loss/len(trainloader.sampler)\n",
        "    valid_loss = valid_loss/len(validloader.sampler)\n",
        "  \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "      epoch+1, \n",
        "      train_loss,\n",
        "      valid_loss\n",
        "      ))\n",
        "    \n",
        "    # save model only if validation loss has decreased\n",
        "    if valid_loss <= min_valid_loss:\n",
        "        print('Validation loss decreased ({} --> {}).  Saving model ...'.format(\n",
        "        min_valid_loss,\n",
        "        valid_loss))\n",
        "        torch.save(new_model.state_dict(), 'model1.pt')\n",
        "        min_valid_loss = valid_loss\n",
        "    initial_accuracy = test_accuracy(new_model, testloader, criterion)\n",
        "    print(initial_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFbLcyPOcT1z"
      },
      "source": [
        "initial_accuracy = test_accuracy(new_model, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMF-vH6SUSBn"
      },
      "source": [
        "# Test Accuracy after Iterative prunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpT55d5-TUlp"
      },
      "source": [
        "new_model1= MyNet().to(device=device)\n",
        "# new_model1=new_model\n",
        "new_model1.load_state_dict(torch.load('model1.pt'))\n",
        "for name, module in new_model1.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.01)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.02)\n",
        "\n",
        "print(dict(new_model1.named_buffers()).keys())  # to verify that all masks exist\n",
        "initial_accuracy = test_accuracy(new_model1, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WtBSw5Rf6zs"
      },
      "source": [
        "new_model1= MyNet().to(device=device)\n",
        "# new_model1=new_model\n",
        "new_model1.load_state_dict(torch.load('model1.pt'))\n",
        "for name, module in new_model1.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.1)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "\n",
        "print(dict(new_model1.named_buffers()).keys())  # to verify that all masks exist\n",
        "initial_accuracy = test_accuracy(new_model1, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ogC0aCnf9YD"
      },
      "source": [
        "new_model1= MyNet().to(device=device)\n",
        "# new_model1=new_model\n",
        "new_model1.load_state_dict(torch.load('model1.pt'))\n",
        "for name, module in new_model1.named_modules():\n",
        "    # prune 20% of connections in all 2D-conv layers\n",
        "    if isinstance(module, torch.nn.Conv2d):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
        "    # prune 40% of connections in all linear layers\n",
        "    elif isinstance(module, torch.nn.Linear):\n",
        "        prune.l1_unstructured(module, name='weight', amount=0.4)\n",
        "\n",
        "print(dict(new_model1.named_buffers()).keys())  # to verify that all masks exist\n",
        "initial_accuracy = test_accuracy(new_model1, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDIctulWUTz"
      },
      "source": [
        "# test after global prunning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDACzU32i0Rr"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Dphw3MFi4Et"
      },
      "source": [
        "torch.cuda.empty_cache() # PyTorch thing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqgwCzriWNcq"
      },
      "source": [
        "new_model2= MyNet().to(device=device)\n",
        "new_model2.load_state_dict(torch.load('model1.pt'))\n",
        "# new_model2=new_model\n",
        "model = new_model2\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    # (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.15,   \n",
        ")\n",
        "initial_accuracy = test_accuracy(new_model2, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2UucLmsWfI6"
      },
      "source": [
        "\n",
        "new_model2= MyNet().to(device=device)\n",
        "new_model2.load_state_dict(torch.load('model1.pt'))\n",
        "# new_model2=new_model\n",
        "model = new_model2\n",
        "\n",
        "parameters_to_prune = (\n",
        "    (model.conv1, 'weight'),\n",
        "    (model.conv2, 'weight'),\n",
        "    (model.fc1, 'weight'),\n",
        "    (model.fc2, 'weight'),\n",
        "    # (model.fc3, 'weight'),\n",
        ")\n",
        "\n",
        "prune.global_unstructured(\n",
        "    parameters_to_prune,\n",
        "    pruning_method=prune.L1Unstructured,\n",
        "    amount=0.15,   \n",
        ")\n",
        "initial_accuracy = test_accuracy(new_model2, testloader, criterion)\n",
        "print(initial_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}